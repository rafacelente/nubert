{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3de8289-3336-4541-a1e9-582230cdf091",
   "metadata": {},
   "source": [
    "# 04. Modeling - Nubank AI Core Transaction Dataset Interview Project\n",
    "\n",
    "In this section we will go over the theory and practice of the models we will train to extract deep representations of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88776105-0c5f-466c-b5c3-413c9ee57024",
   "metadata": {},
   "source": [
    "## The Language Task == The Sequence Task\n",
    "\n",
    "Modern transformers-based architectures use the self-attention mechanism to capture long-range dependencies and contextual information from sequence data, learning deep representations from these sequences. Although they are applied heavily on text data (leading to models such as GPT, LLaMa and BERT), their core idea applies to any type of sequence data, which includes our task of extracting deep features from transaction sequences.\n",
    "\n",
    "These models learn to extract these representations by being trained on the self-supervised task of token-masking. The idea is that, given an input sequence, some tokens of the sequence will be masked from the model, and his objective is try to reconstruct what are those missing tokens. The way these tokens are masked varies from model to model.\n",
    "\n",
    "GPT-style models' ([1])[https://arxiv.org/pdf/2005.14165] primary objective is unidirectional sequence modeling. It aims to predict the next token in a sequence given all the previous tokens. This works great for language modeling because language is mostly unidirectional, as most languages in the West are written from left to write. This also allows the model to be used as an auto-regressive generator, where, given an input sequence, the model can output the most probable next token and add it to the next input sequence, repeating the process auto-regressively.\n",
    "\n",
    "However, for language-understanding downstream tasks such as sequence classification, bidirectional models such as BERT ([2])[https://arxiv.org/pdf/1810.04805]  have an edge. BERT employs the Masked Language Modeling (MLM) objective, which involves involves randomly masking 15% of the input tokens and training the model to predict these masked tokens based on both left and right context.\n",
    "\n",
    "## Modeling Tabular Sequences with BERT\n",
    "\n",
    "The TabFormer paper ([3])[https://arxiv.org/pdf/2011.01843] by IBM research introduced the idea of modeling tabular time series data through a language modeling task. In the paper, they introduce TabBERT, a model that can be pre-trained end-to-end for representation learning of tabular time series data, which can then be fine-tuned for specific tasks such as classification and regression.\n",
    "\n",
    "Their insight was that through the *language metaphor*, they can quantize the continuous fields and define a finite vocabulary for the features of a given tabular series, which can then be concatenated and trained as a sequence, much like a NLP task.\n",
    "\n",
    "However, their approach didn't take into consideration the introduction of text data in the fields. Instead, they used only categorical and numerical features for their approach, adding new tokens based on the categorical and quantized values of the tables. While this achieves great results, it misses 2 opportunities:\n",
    "\n",
    "1. Text data can be extremely valuable, allowing the model can learn representations from text data that can correlate with non-text features.\n",
    "2. By using a whole new tokenizer, the available pre-trained-on-text models available become obsolete. By including text data, we can leverage the already learned representations from the pre-training.\n",
    "\n",
    "In the dataset provided by Nubank, most of the fields are text data, which can be used to extract deep representations of each transaction and their sequences. Therefore for training NuBERT, we will include the these text fields as we've discussed in Section 3 - Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3adce45-9bbc-4c7f-92a9-834abd3b4f80",
   "metadata": {},
   "source": [
    "### Training Pipeline\n",
    "\n",
    "For training, we will use the tokenized sequences based on the cleaned dataset that we've mentioned before. For the training framework, we will use Hugging Face's `transformer` and `datasets`, which give us some high-level interfaces to easily model these languages without much boilerplate code. The model we will use is the `distilbert`, which is a distilled version of BERT with 40% less parameters. The training logs are logged with WandB. Here's a compilation of the hyperparameters used for this sample training run.\n",
    "\n",
    "*note: this training run done here is only for demonstration purposes. The experiment training runs were done either through the nubert scripts or the next notebooks*.\n",
    "\n",
    "- Number of transactions per sequence: 5\n",
    "- Stride: 1\n",
    "- Amount Bins: 20\n",
    "- Number of training epochs: 1\n",
    "- Train/val batch sizes: 128\n",
    "- Max sequence length: 512\n",
    "- Train/val/test split: [0.9, 0.1, 0.1]\n",
    "- Optimizer: AdamW | Beta1 = 0.9, Beta2 = 0.999\n",
    "- Initial learning rate: 5e-5\n",
    "- Learning rate schedular: Linear\n",
    "- Warmup steps: 1000\n",
    "- Gradient accumulation steps: 1\n",
    "- Use bf16: True\n",
    "\n",
    "At the time of running this, this model is trained on a A6000-45GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ccf959-3bad-4669-9179-21d10084eaa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T16:36:38.534811Z",
     "iopub.status.busy": "2024-10-19T16:36:38.533848Z",
     "iopub.status.idle": "2024-10-19T16:36:43.096694Z",
     "shell.execute_reply": "2024-10-19T16:36:43.096142Z",
     "shell.execute_reply.started": "2024-10-19T16:36:38.534778Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nubert import NuDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0802ee-8398-4a01-bd52-31bccf8113e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T16:36:43.098283Z",
     "iopub.status.busy": "2024-10-19T16:36:43.097898Z",
     "iopub.status.idle": "2024-10-19T16:36:43.102283Z",
     "shell.execute_reply": "2024-10-19T16:36:43.101728Z",
     "shell.execute_reply.started": "2024-10-19T16:36:43.098264Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_size=0.1, val_size=0.1, seed=42):\n",
    "    train_val, test = train_test_split(dataset, test_size=test_size, random_state=seed)    \n",
    "    train, val = train_test_split(train_val, test_size=val_size / (1 - test_size), random_state=seed)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def create_hf_dataset(data):\n",
    "    return Dataset.from_dict({\"input_ids\": data})\n",
    "\n",
    "def resize_model_embeddings(model, tokenizer):\n",
    "    \"\"\"Resize the model's embeddings to match the tokenizer's vocabulary size.\"\"\"\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdda2f8-d9cb-41ad-89a3-7182cfa66870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T16:36:43.103271Z",
     "iopub.status.busy": "2024-10-19T16:36:43.103116Z",
     "iopub.status.idle": "2024-10-19T16:52:07.142595Z",
     "shell.execute_reply": "2024-10-19T16:52:07.141366Z",
     "shell.execute_reply.started": "2024-10-19T16:36:43.103257Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:61: DtypeWarning: Columns (12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path.join(root, f\"{fname}.csv\"))\n",
      "/usr/local/lib/python3.9/dist-packages/nubert/utils/dataset_utils.py:81: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
      "100%|██████████| 111/111 [15:17<00:00,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Dataset Summary:\n",
      "num_samples: 427646\n",
      "num_tokens: 92343162\n",
      "num_features: 9\n",
      "features: Index(['Agency Name', 'Vendor', 'Merchant Category Code (MCC)', 'Timestamp',\n",
      "       'Amount', 'Transaction Date', 'Original Amount', 'Amount Min',\n",
      "       'Amount Max'],\n",
      "      dtype='object')\n",
      "num_transaction_sequences: 5\n",
      "max_seq_len: 512\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "dataset_path = \"/notebooks/nubank/nugpt/analyses/nubank-2013-2014/\"\n",
    "num_transactions = 5\n",
    "stride = 1\n",
    "max_length = 512\n",
    "num_amount_bins = 20\n",
    "\n",
    "full_dataset = NuDataset.from_raw_data(\n",
    "                    root=dataset_path,\n",
    "                    fname=\"nubank_raw\",\n",
    "                    num_bins=num_amount_bins,\n",
    "                    model_name=model_name,\n",
    "                    num_transaction_sequences=num_transactions,\n",
    "                    max_seq_len=max_length,\n",
    "                    stride=stride,\n",
    "                )\n",
    "summary = full_dataset.get_summary(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7cf9d7a-795f-49b4-bb81-fad54d36662b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T16:52:07.144012Z",
     "iopub.status.busy": "2024-10-19T16:52:07.143800Z",
     "iopub.status.idle": "2024-10-19T16:52:20.587692Z",
     "shell.execute_reply": "2024-10-19T16:52:20.587206Z",
     "shell.execute_reply.started": "2024-10-19T16:52:07.144012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7199d00503894950920376be2f4e277c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = \"/notebooks/nuvank/nubert\"\n",
    "tokenizer = full_dataset.tokenizer.base_tokenizer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "model = resize_model_embeddings(model, tokenizer)\n",
    "\n",
    "train_data, val_data, test_data = split_dataset(full_dataset.data)\n",
    "\n",
    "train_dataset = create_hf_dataset(train_data)\n",
    "val_dataset = create_hf_dataset(val_data)\n",
    "test_dataset = create_hf_dataset(test_data)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939dac78-0307-4892-9408-5e968245d8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T16:52:20.589294Z",
     "iopub.status.busy": "2024-10-19T16:52:20.589114Z",
     "iopub.status.idle": "2024-10-19T16:52:21.595997Z",
     "shell.execute_reply": "2024-10-19T16:52:21.595393Z",
     "shell.execute_reply.started": "2024-10-19T16:52:20.589279Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"nubert\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "\n",
    "run_name = f\"nubert-distil-transactions-{num_transactions}-stride-{stride}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1.0,\n",
    "        per_device_train_batch_size=128,\n",
    "        per_device_eval_batch_size=128,\n",
    "        learning_rate=5e-5,\n",
    "        bf16=True,\n",
    "        save_total_limit=1,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=run_name,\n",
    "        save_strategy = \"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad76fc01-d4cc-4352-9d6a-cc79a7fee7cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T16:52:21.598556Z",
     "iopub.status.busy": "2024-10-19T16:52:21.598417Z",
     "iopub.status.idle": "2024-10-19T17:19:26.886080Z",
     "shell.execute_reply": "2024-10-19T17:19:26.885186Z",
     "shell.execute_reply.started": "2024-10-19T16:52:21.598541Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/nubank/nubert/analyses/nubank-2013-2014/wandb/run-20241019_165554-eoqe2yz9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rafaelmcelente/nubert/runs/eoqe2yz9' target=\"_blank\">nubert-distil-transactions-5-stride-1</a></strong> to <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rafaelmcelente/nubert/runs/eoqe2yz9' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/eoqe2yz9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2673' max='2673' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2673/2673 23:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.215927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/notebooks/nuvank/nubert/tokenizer_config.json',\n",
       " '/notebooks/nuvank/nubert/special_tokens_map.json',\n",
       " '/notebooks/nuvank/nubert/vocab.txt',\n",
       " '/notebooks/nuvank/nubert/added_tokens.json',\n",
       " '/notebooks/nuvank/nubert/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
