{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc9ccdb-127f-4c5a-82b7-aa7b18441bf4",
   "metadata": {},
   "source": [
    "# 03. Exploratory Analysis - Nubank AI Core Transaction Dataset Interview Project\n",
    "\n",
    "In this section we will explore the tokenization and organization of our transactions to create sequence samples for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4926c98a-0c0c-49f2-8f65-279a5f8dfe2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T21:10:42.041326Z",
     "iopub.status.busy": "2024-10-17T21:10:42.040711Z",
     "iopub.status.idle": "2024-10-17T21:10:42.045314Z",
     "shell.execute_reply": "2024-10-17T21:10:42.044745Z",
     "shell.execute_reply.started": "2024-10-17T21:10:42.041303Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme()\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#8A05BE\", \"#A5D936\", \"#191919\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d29ec35-83fb-4e03-b219-e60c8469aef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T21:10:42.746989Z",
     "iopub.status.busy": "2024-10-17T21:10:42.746068Z",
     "iopub.status.idle": "2024-10-17T21:10:43.090329Z",
     "shell.execute_reply": "2024-10-17T21:10:43.089781Z",
     "shell.execute_reply.started": "2024-10-17T21:10:42.746954Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./nubank_checkpoint_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e65f953-310b-4c8b-a2c8-9593bb900362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T21:10:43.705742Z",
     "iopub.status.busy": "2024-10-17T21:10:43.705455Z",
     "iopub.status.idle": "2024-10-17T21:10:43.713908Z",
     "shell.execute_reply": "2024-10-17T21:10:43.713328Z",
     "shell.execute_reply.started": "2024-10-17T21:10:43.705722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agency Name</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Vendor</th>\n",
       "      <th>Transaction Date</th>\n",
       "      <th>Merchant Category Code (MCC)</th>\n",
       "      <th>Original Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OKLAHOMA STATE UNIVERSITY</td>\n",
       "      <td>10</td>\n",
       "      <td>NACAS</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>CHARITABLE AND SOCIAL SERVICE ORGANIZATIONS</td>\n",
       "      <td>890.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OKLAHOMA STATE UNIVERSITY</td>\n",
       "      <td>9</td>\n",
       "      <td>SHERATON HOTEL</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>SHERATON</td>\n",
       "      <td>368.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OKLAHOMA STATE UNIVERSITY</td>\n",
       "      <td>8</td>\n",
       "      <td>SEARS.COM 9300</td>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>DIRCT MARKETING/DIRCT MARKETERS--NOT ELSEWHERE...</td>\n",
       "      <td>165.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OKLAHOMA STATE UNIVERSITY</td>\n",
       "      <td>7</td>\n",
       "      <td>WAL-MART #0137</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>GROCERY STORES,AND SUPERMARKETS</td>\n",
       "      <td>96.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OKLAHOMA STATE UNIVERSITY</td>\n",
       "      <td>8</td>\n",
       "      <td>STAPLES DIRECT</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>STATIONERY, OFFICE SUPPLIES, PRINTING AND WRIT...</td>\n",
       "      <td>125.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Agency Name  Amount          Vendor Transaction Date  \\\n",
       "0  OKLAHOMA STATE UNIVERSITY      10           NACAS       2013-07-30   \n",
       "1  OKLAHOMA STATE UNIVERSITY       9  SHERATON HOTEL       2013-07-30   \n",
       "2  OKLAHOMA STATE UNIVERSITY       8  SEARS.COM 9300       2013-07-29   \n",
       "3  OKLAHOMA STATE UNIVERSITY       7  WAL-MART #0137       2013-07-30   \n",
       "4  OKLAHOMA STATE UNIVERSITY       8  STAPLES DIRECT       2013-07-30   \n",
       "\n",
       "                        Merchant Category Code (MCC)  Original Amount  \n",
       "0        CHARITABLE AND SOCIAL SERVICE ORGANIZATIONS           890.00  \n",
       "1                                           SHERATON           368.96  \n",
       "2  DIRCT MARKETING/DIRCT MARKETERS--NOT ELSEWHERE...           165.82  \n",
       "3                    GROCERY STORES,AND SUPERMARKETS            96.39  \n",
       "4  STATIONERY, OFFICE SUPPLIES, PRINTING AND WRIT...           125.96  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66d6a4-751d-45d9-8f72-f8cc17a5f7c8",
   "metadata": {},
   "source": [
    "## From transactions to sequence of transactions\n",
    "\n",
    "Our idea here will be to create model that can extract deep representations from a sequence of transactions from a given agency.\n",
    "\n",
    "If we have a long sequence of previous transactions, we can maybe start to understand what are the spending habits of these agencies. The model will use both text relations and categorical data to understand deep features of the sequences.\n",
    "\n",
    "### Encoding transactions\n",
    "\n",
    "Let's imagine we have a small dataset of a single agency with only 5 transaction samples.\n",
    "\n",
    "| Agency Name      | Vendor             | Merchant Category Code (MCC)                       | Timestamp | Amount |\n",
    "|------------------|---------------------|---------------------------------------------------|-----|------------|\n",
    "| ATTORNEY GENERAL | STAPLES             | STATIONERY, OFFICE SUPPLIES, PRINTING AND WRIT... | 0   | 7          |\n",
    "| ATTORNEY GENERAL | DMI  DELL K-12/GOVT | COMPUTERS, COMPUTER PERIPHERAL EQUIPMENT, SOFT... | 0   | 10         |\n",
    "| ATTORNEY GENERAL | STAPLES             | STATIONERY, OFFICE SUPPLIES, PRINTING AND WRIT... | 1   | 6          |\n",
    "| ATTORNEY GENERAL | FIZZ-O WATER        | MISCELLANEOUS AND SPECIALTY RETAIL STORES         | 1   | 7          |\n",
    "| ATTORNEY GENERAL | VERITEXT CORP       | PROFESSIONAL SERVICES NOT ELSEWHERE CLASSIFIED    | 1   | 7          |\n",
    "\n",
    "Since language models work on tokenized sequence data, we have to find a way to encode these transactions into a text sequence which can then be tokenized by a known tokenizer. One way we can do this is encode each row the same way as we would if we were writing in a sentence. For example, if we have the following transaction:\n",
    "\n",
    "| AgencyName       | Amount              | Vendor                                            | MCC | Timestsamp |\n",
    "|------------------|---------------------|---------------------------------------------------|-----|------------|\n",
    "| ATTORNEY GENERAL | STAPLES             | STATIONERY, OFFICE SUPPLIES, PRINTING AND WRIT... | 0   | 7          |\n",
    "\n",
    "we can encode it as:\n",
    "\n",
    "```\n",
    "Agency Name: ATTORNEY GENERAL, Vendor: STAPLES, Merchant Category Code (MCC): STATIONERY, OFFICE SUPPLIES, PRINTING AND WRIT..., Timestamp: 0, Amount: 7\n",
    "```\n",
    "\n",
    "This sentence can then be tokenized and fed into a language model. However, this doesn't fully capture both the tabular and sequential nature of the transaction. To do that, we can use special tokens to organize our data into a sequence that, instead of representing a sentence, represents a *transaction*. The BERT model, for example, has 2 special tokens: `[CLS]`, which can be used to represent the start of a transaction; and `[SEP]`, which can be used to represent the separation of fields.\n",
    "\n",
    "```\n",
    "[CLS] Agency Name: ATTORNEY GENERAL [SEP] Vendor: STAPLES [SEP] Merchant Category Code (MCC): STATIONERY, OFFICE SUPPLIES, PRINTING AND WRIT... [SEP] Timestamp: 0 [SEP] Amount: 7 [SEP] [SEP]\n",
    "```\n",
    "\n",
    "This way we can encode the information more clearly, indicating expliclity where the information from each transaction starts. This allows us also to differentiate between different transactions.\n",
    "\n",
    "### Encoding transaction sequences\n",
    "\n",
    "\n",
    "Now that we can encode a single transaction, we can do the same for a sequence of transactions. Suppose we have a time-series sequence of 2 transactions:\n",
    "\n",
    "| Agency Name      | Vendor             | Merchant Category Code (MCC)                       | Timestamp | Amount |\n",
    "|------------------|---------------------|---------------------------------------------------|-----|------------|\n",
    "| ATTORNEY GENERAL | STAPLES             | STATIONERY, OFFICE SUPPLIES, PRINTING AND WRIT... | 0   | 7          |\n",
    "| ATTORNEY GENERAL | DMI  DELL K-12/GOVT | COMPUTERS, COMPUTER PERIPHERAL EQUIPMENT, SOFT... | 0   | 10         |\n",
    "\n",
    "We can encode this sequence by encoding each transaction individually and then concatenating the results into a single time-dependant sample sequence.\n",
    "\n",
    "\n",
    "```\n",
    "[CLS] Agency Name: ATTORNEY GENERAL [SEP] Vendor: STAPLES [SEP] Merchant Category Code (MCC): STATIONERY, OFFICE SUPPLIES, PRINTING AND WRIT... [SEP] Timestamp: 0 [SEP] Amount: 7 [SEP] [SEP] [CLS] Agency Name: ATTORNEY GENERAL [SEP] Vendor: DMI  DELL K-12/GOVT [SEP] Merchant Category Code (MCC): COMPUTERS, COMPUTER PERIPHERAL EQUIPMENT, SOFT... [SEP] Timestamp: 0 [SEP] Amount: 10 [SEP] [SEP]\n",
    "```\n",
    "\n",
    "This encoding allows us to express some interesting things:\n",
    "\n",
    "1. Through the `[CLS]` and `[SEP]` tokens we can encode clearly for each of the transactions where each of the fields is located.\n",
    "2. Through the positional representation of these sequences we can encode a time-dependant constraint of our data.\n",
    "3. Through the text fields we can encode contextual representations of the values, and using pre-trained language models we can extract deep features more efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385947ea-f7dc-4103-9461-592b7eca733a",
   "metadata": {},
   "source": [
    "### Reducing token selection variability\n",
    "\n",
    "Although in the data cleaning process we have decided to quantize and categorize our numerical/time features, once this is encoded as text by the tokenizer, these features may be represented by a variety of tokens. That is, the quantized amoun `7` may be represented, for example, by the tokens `7`, ` 7`, `7 `, `: 7` and so forth. Therefore, to reduce this variability in the representation of our numerical values, we can include added tokens into a vocabulary that represent exactly the categorical tokens that we want to model. \n",
    "\n",
    "For example, if we know we have 20 categorical values for `Amount`, we can create 20 new special tokens `<Amount_bin_0>, <Amount_bins_1> ... <Amount_bin_19>` and add them to the our vocabulary. Then, when we encode our dataset, we translate our `Amount` data into these categorical text representations which will then be tokenized as our new tokens. This way, our model learns to output only 19 possible tokens for `Amount`, instead of a lot more possible representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db6c8d-1119-43ee-aa1a-28f7abfadd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
