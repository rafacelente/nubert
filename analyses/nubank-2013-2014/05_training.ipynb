{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3de8289-3336-4541-a1e9-582230cdf091",
   "metadata": {},
   "source": [
    "# 05. Modeling - Nubank AI Core Transaction Dataset Interview Project\n",
    "\n",
    "In this section we will train our model with different hyperparameters and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08bbcbeb-df4f-40ee-ba75-0fa604230fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T17:00:11.906018Z",
     "iopub.status.busy": "2024-10-22T17:00:11.905294Z",
     "iopub.status.idle": "2024-10-22T17:00:19.024930Z",
     "shell.execute_reply": "2024-10-22T17:00:19.024204Z",
     "shell.execute_reply.started": "2024-10-22T17:00:11.905987Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nubert.datasets import NuDataset\n",
    "from nubert.config import NubertPreTrainConfig, TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0802ee-8398-4a01-bd52-31bccf8113e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T17:00:19.026581Z",
     "iopub.status.busy": "2024-10-22T17:00:19.026172Z",
     "iopub.status.idle": "2024-10-22T17:00:19.030909Z",
     "shell.execute_reply": "2024-10-22T17:00:19.030375Z",
     "shell.execute_reply.started": "2024-10-22T17:00:19.026579Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_size=0.1, val_size=0.1, seed=42):\n",
    "    train_val, test = train_test_split(dataset, test_size=test_size, random_state=seed)    \n",
    "    train, val = train_test_split(train_val, test_size=val_size / (1 - test_size), random_state=seed)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def create_hf_dataset(data):\n",
    "    return Dataset.from_dict({\"input_ids\": data})\n",
    "\n",
    "def resize_model_embeddings(model, tokenizer):\n",
    "    \"\"\"Resize the model's embeddings to match the tokenizer's vocabulary size.\"\"\"\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe064b3-8be3-4e0c-8e4a-71f7bf1a9135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T17:00:19.032130Z",
     "iopub.status.busy": "2024-10-22T17:00:19.031922Z",
     "iopub.status.idle": "2024-10-22T17:00:19.359268Z",
     "shell.execute_reply": "2024-10-22T17:00:19.358687Z",
     "shell.execute_reply.started": "2024-10-22T17:00:19.032114Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "def train_model(\n",
    "    dataset,\n",
    "    config: NubertPreTrainConfig,\n",
    "    ):\n",
    "    model = AutoModelForMaskedLM.from_pretrained(config.model_name)\n",
    "    tokenizer = dataset.tokenizer.base_tokenizer\n",
    "\n",
    "    tokenizer.save_pretrained(config.trainer.output_dir)\n",
    "    model = resize_model_embeddings(model, tokenizer)\n",
    "\n",
    "    train_data, val_data, test_data = split_dataset(dataset.data)\n",
    "\n",
    "    train_dataset = create_hf_dataset(train_data)\n",
    "    val_dataset = create_hf_dataset(val_data)\n",
    "    test_dataset = create_hf_dataset(test_data)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        **config.trainer.model_dump()\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(config.trainer.output_dir)\n",
    "    wandb.finish()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55607a3b-1627-4919-909d-c1c0f021265c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T17:00:19.360459Z",
     "iopub.status.busy": "2024-10-22T17:00:19.360288Z",
     "iopub.status.idle": "2024-10-22T17:25:24.565844Z",
     "shell.execute_reply": "2024-10-22T17:25:24.562524Z",
     "shell.execute_reply.started": "2024-10-22T17:00:19.360438Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:103: DtypeWarning: Columns (12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path.join(root, f\"{fname}.csv\"))\n",
      "/usr/local/lib/python3.9/dist-packages/nubert/utils/dataset_utils.py:81: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1341ee24319a468fa8191a9a421b5c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3415901f2f214253954a8706a5ca0a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837ae8eaed5b45a7acdb326f845d0186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157ac06137f648c69042359539f2f63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 104/110 [24:54<01:26, 14.37s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 28\u001b[0m\n\u001b[1;32m     14\u001b[0m trainer_config \u001b[38;5;241m=\u001b[39m TrainerConfig(\n\u001b[1;32m     15\u001b[0m     per_device_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     16\u001b[0m     per_device_eval_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m config \u001b[38;5;241m=\u001b[39m NubertPreTrainConfig(\n\u001b[1;32m     19\u001b[0m     run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/notebooks/nubank/nubert/analyses/nubank-2013-2014\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     randomize_column_order \u001b[38;5;241m=\u001b[39m randomize_column_order,\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNuDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m train_model(dataset\u001b[38;5;241m=\u001b[39mfull_dataset, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:56\u001b[0m, in \u001b[0;36mNuDataset.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_config\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config: NubertPreTrainConfig):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mfrom_cleaned_data:\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_raw_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilter_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns_to_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns_to_drop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43magency_names_to_remove\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magency_names_to_remove\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_transaction_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_transactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandomize_column_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandomize_column_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_cleaned_data(\n\u001b[1;32m     70\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     71\u001b[0m         root\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdataset_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m         nrows\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnrows\n\u001b[1;32m     78\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:115\u001b[0m, in \u001b[0;36mNuDataset.from_raw_data\u001b[0;34m(cls, fname, root, filter_list, num_bins, columns_to_drop, agency_names_to_remove, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(path\u001b[38;5;241m.\u001b[39mjoin(root, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.cleaned.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    114\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved cleaned data to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.cleaned.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.cleaned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:51\u001b[0m, in \u001b[0;36mNuDataset.__init__\u001b[0;34m(self, model_name, root, fname, fextension, num_transaction_sequences, max_seq_len, nrows, stride, use_pretrained_tokenizer, randomize_column_order)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_pretrained_tokenizer:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_tokenizer()\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:181\u001b[0m, in \u001b[0;36mNuDataset.prepare_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    180\u001b[0m flattened_sequence \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (_, transaction) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(group\u001b[38;5;241m.\u001b[39miterrows()):\n\u001b[1;32m    182\u001b[0m     column_order \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgency Name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVendor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMerchant Category Code (MCC)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandomize_column_order:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[1;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py:593\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    590\u001b[0m         data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n\u001b[1;32m    592\u001b[0m NDFrame\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_axis(\u001b[38;5;241m0\u001b[39m, index)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py:6312\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6307\u001b[0m \u001b[38;5;66;03m# first try regular attribute access via __getattribute__, so that\u001b[39;00m\n\u001b[1;32m   6308\u001b[0m \u001b[38;5;66;03m# e.g. ``obj.x`` and ``obj.x = 4`` will always reference/modify\u001b[39;00m\n\u001b[1;32m   6309\u001b[0m \u001b[38;5;66;03m# the same attribute.\u001b[39;00m\n\u001b[1;32m   6311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6312\u001b[0m     \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m   6314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py:782\u001b[0m, in \u001b[0;36mSeries.name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Hashable:\n\u001b[1;32m    736\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m    Return the name of the Series.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m    'Even Numbers'\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py:6284\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6280\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mallows_duplicate_labels \u001b[38;5;241m=\u001b[39m allows_duplicate_labels\n\u001b[1;32m   6282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 6284\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   6285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   6286\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6287\u001b[0m \u001b[38;5;124;03m    After regular attribute access, try looking up the name\u001b[39;00m\n\u001b[1;32m   6288\u001b[0m \u001b[38;5;124;03m    This allows simpler access to columns for interactive use.\u001b[39;00m\n\u001b[1;32m   6289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   6290\u001b[0m     \u001b[38;5;66;03m# Note: obj.x will always call obj.__getattribute__('x') prior to\u001b[39;00m\n\u001b[1;32m   6291\u001b[0m     \u001b[38;5;66;03m# calling obj.__getattr__('x').\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"nubert\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "\n",
    "\n",
    "num_transactions_to_test = [10]\n",
    "stride_to_test = [1]\n",
    "num_bins_to_test = [15]\n",
    "randomized_to_test = [False]\n",
    "\n",
    "for num_transactions in num_transactions_to_test:\n",
    "    for stride in stride_to_test:\n",
    "        for num_bins in num_bins_to_test:\n",
    "            for randomize_column_order in randomized_to_test:\n",
    "                trainer_config = TrainerConfig(\n",
    "                    per_device_train_batch_size = 64,\n",
    "                    per_device_eval_batch_size = 64,\n",
    "                )\n",
    "                config = NubertPreTrainConfig(\n",
    "                    run_name=\"\",\n",
    "                    dataset_path = \"/notebooks/nubank/nubert/analyses/nubank-2013-2014\",\n",
    "                    file_name = \"nubank_raw\",\n",
    "                    num_transactions = num_transactions,\n",
    "                    stride = stride,\n",
    "                    num_bins = num_bins,\n",
    "                    trainer=trainer_config,\n",
    "                    randomize_column_order = randomize_column_order,\n",
    "                )\n",
    "                full_dataset = NuDataset.from_config(config)\n",
    "                train_model(dataset=full_dataset, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee6458-390c-47ea-b99d-0120313c50a9",
   "metadata": {},
   "source": [
    "Cool, we have trained our models. The evaluation of their performances will be done outside of this notebook because the results are logged on Weights and Biases.\n",
    "\n",
    "### Fine-tuning\n",
    "\n",
    "For the continuation of this notebook (fine-tuning for amount prediction), head on over to the amount directory on analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27b777-77a6-4346-9fb6-1a02492a69dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
