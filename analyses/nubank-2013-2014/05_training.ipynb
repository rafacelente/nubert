{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3de8289-3336-4541-a1e9-582230cdf091",
   "metadata": {},
   "source": [
    "# 05. Modeling - Nubank AI Core Transaction Dataset Interview Project\n",
    "\n",
    "In this section we will train our model with different hyperparameters and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08bbcbeb-df4f-40ee-ba75-0fa604230fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T17:10:01.092367Z",
     "iopub.status.busy": "2024-10-20T17:10:01.091755Z",
     "iopub.status.idle": "2024-10-20T17:10:09.050061Z",
     "shell.execute_reply": "2024-10-20T17:10:09.049552Z",
     "shell.execute_reply.started": "2024-10-20T17:10:01.092344Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nubert.datasets import NuDataset\n",
    "from nubert.config import NubertPreTrainConfig, TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0802ee-8398-4a01-bd52-31bccf8113e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T17:10:09.051951Z",
     "iopub.status.busy": "2024-10-20T17:10:09.051045Z",
     "iopub.status.idle": "2024-10-20T17:10:09.056059Z",
     "shell.execute_reply": "2024-10-20T17:10:09.055317Z",
     "shell.execute_reply.started": "2024-10-20T17:10:09.051929Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_size=0.1, val_size=0.1, seed=42):\n",
    "    train_val, test = train_test_split(dataset, test_size=test_size, random_state=seed)    \n",
    "    train, val = train_test_split(train_val, test_size=val_size / (1 - test_size), random_state=seed)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def create_hf_dataset(data):\n",
    "    return Dataset.from_dict({\"input_ids\": data})\n",
    "\n",
    "def resize_model_embeddings(model, tokenizer):\n",
    "    \"\"\"Resize the model's embeddings to match the tokenizer's vocabulary size.\"\"\"\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe064b3-8be3-4e0c-8e4a-71f7bf1a9135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T17:10:09.057704Z",
     "iopub.status.busy": "2024-10-20T17:10:09.057080Z",
     "iopub.status.idle": "2024-10-20T17:10:09.355772Z",
     "shell.execute_reply": "2024-10-20T17:10:09.354859Z",
     "shell.execute_reply.started": "2024-10-20T17:10:09.057684Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "def train_model(\n",
    "    dataset,\n",
    "    config: NubertPreTrainConfig,\n",
    "    ):\n",
    "    model = AutoModelForMaskedLM.from_pretrained(config.model_name)\n",
    "    tokenizer = dataset.tokenizer.base_tokenizer\n",
    "\n",
    "    tokenizer.save_pretrained(config.trainer.output_dir)\n",
    "    model = resize_model_embeddings(model, tokenizer)\n",
    "\n",
    "    train_data, val_data, test_data = split_dataset(dataset.data)\n",
    "\n",
    "    train_dataset = create_hf_dataset(train_data)\n",
    "    val_dataset = create_hf_dataset(val_data)\n",
    "    test_dataset = create_hf_dataset(test_data)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        **config.trainer.model_dump()\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(config.trainer.output_dir)\n",
    "    wandb.finish()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdda2f8-d9cb-41ad-89a3-7182cfa66870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T17:10:09.357212Z",
     "iopub.status.busy": "2024-10-20T17:10:09.356966Z",
     "iopub.status.idle": "2024-10-20T17:10:09.361407Z",
     "shell.execute_reply": "2024-10-20T17:10:09.360748Z",
     "shell.execute_reply.started": "2024-10-20T17:10:09.357185Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig(\n",
    "    per_device_train_batch_size = 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    ")\n",
    "\n",
    "config = NubertPreTrainConfig(\n",
    "    dataset_path = \"/notebooks/nubank/nubert/analyses/nubank-2013-2014\",\n",
    "    file_name = \"nubank_raw\",\n",
    "    num_transactions = 5,\n",
    "    stride = 1,\n",
    "    num_bins = 20,\n",
    "    trainer=trainer_config,\n",
    ")\n",
    "\n",
    "# full_dataset = NuDataset.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7cf9d7a-795f-49b4-bb81-fad54d36662b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T17:10:09.363439Z",
     "iopub.status.busy": "2024-10-20T17:10:09.362634Z",
     "iopub.status.idle": "2024-10-20T17:10:09.365854Z",
     "shell.execute_reply": "2024-10-20T17:10:09.365507Z",
     "shell.execute_reply.started": "2024-10-20T17:10:09.363419Z"
    }
   },
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_PROJECT\"] = \"nubert\"\n",
    "# os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "\n",
    "\n",
    "# num_transactions_to_test = [7]\n",
    "# stride_to_test = [2]\n",
    "# num_bins_to_test = [15, 20]\n",
    "\n",
    "# for num_transactions in num_transactions_to_test:\n",
    "#     for stride in stride_to_test:\n",
    "#         for num_bins in num_bins_to_test:\n",
    "#             trainer_config = TrainerConfig(\n",
    "#                 per_device_train_batch_size = 64,\n",
    "#                 per_device_eval_batch_size = 64,\n",
    "#             )\n",
    "#             config = NubertPreTrainConfig(\n",
    "#                 dataset_path = \"/notebooks/nubank/nubert/analyses/nubank-2013-2014\",\n",
    "#                 file_name = \"nubank_raw\",\n",
    "#                 num_transactions = num_transactions,\n",
    "#                 stride = stride,\n",
    "#                 num_bins = num_bins,\n",
    "#                 trainer=trainer_config,\n",
    "#             )\n",
    "#             full_dataset = NuDataset.from_config(config)\n",
    "#             train_model(dataset=full_dataset, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55607a3b-1627-4919-909d-c1c0f021265c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T17:10:09.366741Z",
     "iopub.status.busy": "2024-10-20T17:10:09.366454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:103: DtypeWarning: Columns (12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path.join(root, f\"{fname}.csv\"))\n",
      "/usr/local/lib/python3.9/dist-packages/nubert/utils/dataset_utils.py:81: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedd4287828d46bfa2c71b092d911e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65c62d64203474f9e5a88c51b4836ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac08a56885b4c3b8244f47066cfa94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0572b6edef1641c6a0b956d680a9f85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [30:54<00:00, 16.86s/it]  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563ecf9f171f409db41822c81ed9444f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrafaelmcelente\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/nubank/nubert/analyses/nubank-2013-2014/wandb/run-20241020_174140-m4hr1usu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rafaelmcelente/nubert/runs/m4hr1usu' target=\"_blank\">nubert-distil-transactions-10-stride-1-randomize-True-bins-15</a></strong> to <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rafaelmcelente/nubert/runs/m4hr1usu' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/m4hr1usu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5346' max='5346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5346/5346 44:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.173036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8702c9e921f948a8ada3facf7a709a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='86.992 MB of 255.730 MB uploaded\\r'), FloatProgress(value=0.3401735202757312, max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▇▅▄▃▃▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▂▁▁▁▁▂▂▁▁▂▂▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▆▅▅▃▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▂▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.17304</td></tr><tr><td>eval/runtime</td><td>121.4844</td></tr><tr><td>eval/samples_per_second</td><td>352.012</td></tr><tr><td>eval/steps_per_second</td><td>5.507</td></tr><tr><td>total_flos</td><td>4.534091212924877e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>5346</td></tr><tr><td>train/grad_norm</td><td>0.46306</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.176</td></tr><tr><td>train_loss</td><td>0.2404</td></tr><tr><td>train_runtime</td><td>2693.9618</td></tr><tr><td>train_samples_per_second</td><td>126.991</td></tr><tr><td>train_steps_per_second</td><td>1.984</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nubert-distil-transactions-10-stride-1-randomize-True-bins-15</strong> at: <a href='https://wandb.ai/rafaelmcelente/nubert/runs/m4hr1usu' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/m4hr1usu</a><br/> View project at: <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241020_174140-m4hr1usu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:103: DtypeWarning: Columns (12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path.join(root, f\"{fname}.csv\"))\n",
      "/usr/local/lib/python3.9/dist-packages/nubert/utils/dataset_utils.py:81: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
      "100%|██████████| 110/110 [29:55<00:00, 16.33s/it]  \n",
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/nubank/nubert/analyses/nubank-2013-2014/wandb/run-20241020_185719-7rpuhlsv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rafaelmcelente/nubert/runs/7rpuhlsv' target=\"_blank\">nubert-distil-transactions-10-stride-1-randomize-False-bins-15</a></strong> to <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rafaelmcelente/nubert/runs/7rpuhlsv' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/7rpuhlsv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5346' max='5346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5346/5346 45:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>0.170246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44adb6a9bb754acfb15faa8e8d50f811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='72.758 MB of 255.729 MB uploaded\\r'), FloatProgress(value=0.28451082002111416, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▆▅▂▃▃▃▃▂▃▃▂▂▂▃▂▂▂▂▂▅▂▂▂▂▂▂▂▁▁▂▂▂▁▂▁▁▂▁▂</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▆▆▅▄▄▃▃▃▂▃▃▃▃▃▃▂▂▁▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.17025</td></tr><tr><td>eval/runtime</td><td>123.6607</td></tr><tr><td>eval/samples_per_second</td><td>345.817</td></tr><tr><td>eval/steps_per_second</td><td>5.41</td></tr><tr><td>total_flos</td><td>4.534091212924877e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>5346</td></tr><tr><td>train/grad_norm</td><td>0.55395</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1776</td></tr><tr><td>train_loss</td><td>0.23514</td></tr><tr><td>train_runtime</td><td>2702.733</td></tr><tr><td>train_samples_per_second</td><td>126.579</td></tr><tr><td>train_steps_per_second</td><td>1.978</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nubert-distil-transactions-10-stride-1-randomize-False-bins-15</strong> at: <a href='https://wandb.ai/rafaelmcelente/nubert/runs/7rpuhlsv' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/7rpuhlsv</a><br/> View project at: <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241020_185719-7rpuhlsv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:103: DtypeWarning: Columns (12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path.join(root, f\"{fname}.csv\"))\n",
      "/usr/local/lib/python3.9/dist-packages/nubert/utils/dataset_utils.py:81: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
      "100%|██████████| 110/110 [30:58<00:00, 16.89s/it]  \n",
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/nubank/nubert/analyses/nubank-2013-2014/wandb/run-20241020_201404-vlksm1p5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rafaelmcelente/nubert/runs/vlksm1p5' target=\"_blank\">nubert-distil-transactions-10-stride-1-randomize-True-bins-20</a></strong> to <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rafaelmcelente/nubert/runs/vlksm1p5' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/vlksm1p5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5346' max='5346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5346/5346 44:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.181425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97dfa491a4e4686ada59a70462cc7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='64.633 MB of 255.729 MB uploaded\\r'), FloatProgress(value=0.25273891991492303, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▇▅▇▃▃▄▅▃▃▂▃▃▃▂▂▂▂▁▂▂▂▄▃▂▂▂▂▄▂▁▂▁▂▄▁▃▂▁▁</td></tr><tr><td>train/learning_rate</td><td>██████▇▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▄▃▃▃▃▂▃▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.18143</td></tr><tr><td>eval/runtime</td><td>123.8838</td></tr><tr><td>eval/samples_per_second</td><td>345.195</td></tr><tr><td>eval/steps_per_second</td><td>5.4</td></tr><tr><td>total_flos</td><td>4.534091212924877e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>5346</td></tr><tr><td>train/grad_norm</td><td>0.52447</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1804</td></tr><tr><td>train_loss</td><td>0.24672</td></tr><tr><td>train_runtime</td><td>2692.8274</td></tr><tr><td>train_samples_per_second</td><td>127.045</td></tr><tr><td>train_steps_per_second</td><td>1.985</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nubert-distil-transactions-10-stride-1-randomize-True-bins-20</strong> at: <a href='https://wandb.ai/rafaelmcelente/nubert/runs/vlksm1p5' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/vlksm1p5</a><br/> View project at: <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241020_201404-vlksm1p5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:103: DtypeWarning: Columns (12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path.join(root, f\"{fname}.csv\"))\n",
      "/usr/local/lib/python3.9/dist-packages/nubert/utils/dataset_utils.py:81: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
      "100%|██████████| 110/110 [30:48<00:00, 16.80s/it]  \n",
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/nubank/nubert/analyses/nubank-2013-2014/wandb/run-20241020_213032-ftxdogca</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rafaelmcelente/nubert/runs/ftxdogca' target=\"_blank\">nubert-distil-transactions-10-stride-1-randomize-False-bins-20</a></strong> to <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rafaelmcelente/nubert/runs/ftxdogca' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/ftxdogca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5346' max='5346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5346/5346 45:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.176603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4557b1521f4e79b562ec118b3f8df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='62.430 MB of 255.729 MB uploaded\\r'), FloatProgress(value=0.24412386117087315, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▃▄▂▁▂▁▁▁▁▁▂▁▁▁▂▂▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▅▄▄▄▄▃▃▃▃▄▃▃▃▂▃▂▂▃▂▂▂▁▁▂▁▂▁▂▁▂▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.1766</td></tr><tr><td>eval/runtime</td><td>124.5818</td></tr><tr><td>eval/samples_per_second</td><td>343.26</td></tr><tr><td>eval/steps_per_second</td><td>5.37</td></tr><tr><td>total_flos</td><td>4.534091212924877e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>5346</td></tr><tr><td>train/grad_norm</td><td>0.52085</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1834</td></tr><tr><td>train_loss</td><td>0.24148</td></tr><tr><td>train_runtime</td><td>2704.7891</td></tr><tr><td>train_samples_per_second</td><td>126.483</td></tr><tr><td>train_steps_per_second</td><td>1.976</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nubert-distil-transactions-10-stride-1-randomize-False-bins-20</strong> at: <a href='https://wandb.ai/rafaelmcelente/nubert/runs/ftxdogca' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/ftxdogca</a><br/> View project at: <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241020_213032-ftxdogca/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/nubert/datasets/nudataset.py:103: DtypeWarning: Columns (12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path.join(root, f\"{fname}.csv\"))\n",
      "/usr/local/lib/python3.9/dist-packages/nubert/utils/dataset_utils.py:81: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
      "100%|██████████| 110/110 [31:24<00:00, 17.13s/it]  \n",
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/nubank/nubert/analyses/nubank-2013-2014/wandb/run-20241020_224751-3801yzwf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rafaelmcelente/nubert/runs/3801yzwf' target=\"_blank\">nubert-distil-transactions-10-stride-2-randomize-True-bins-15</a></strong> to <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rafaelmcelente/nubert' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rafaelmcelente/nubert/runs/3801yzwf' target=\"_blank\">https://wandb.ai/rafaelmcelente/nubert/runs/3801yzwf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2182' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2182/5340 17:30 < 25:21, 2.08 it/s, Epoch 0.41/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"nubert\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "\n",
    "\n",
    "num_transactions_to_test = [10]\n",
    "stride_to_test = [2]\n",
    "num_bins_to_test = [15, 20]\n",
    "randomized_to_test = [True, False]\n",
    "\n",
    "for num_transactions in num_transactions_to_test:\n",
    "    for stride in stride_to_test:\n",
    "        for num_bins in num_bins_to_test:\n",
    "            for randomize_column_order in randomized_to_test:\n",
    "                trainer_config = TrainerConfig(\n",
    "                    per_device_train_batch_size = 64,\n",
    "                    per_device_eval_batch_size = 64,\n",
    "                )\n",
    "                config = NubertPreTrainConfig(\n",
    "                    dataset_path = \"/notebooks/nubank/nubert/analyses/nubank-2013-2014\",\n",
    "                    file_name = \"nubank_raw\",\n",
    "                    num_transactions = num_transactions,\n",
    "                    stride = stride,\n",
    "                    num_bins = num_bins,\n",
    "                    trainer=trainer_config,\n",
    "                    randomize_column_order = randomize_column_order,\n",
    "                )\n",
    "                full_dataset = NuDataset.from_config(config)\n",
    "                train_model(dataset=full_dataset, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f92a16-bc6e-4f6a-a89d-c23b78a720e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
