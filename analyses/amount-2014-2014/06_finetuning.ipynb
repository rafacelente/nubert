{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d55650-ebab-41f2-801c-c7761f17ca96",
   "metadata": {},
   "source": [
    "## 06. Finetuning the pre-trained NuBERT model for Amount prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253b0716-7993-4bd1-8031-821b043d8527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T22:07:53.443809Z",
     "iopub.status.busy": "2024-10-22T22:07:53.443214Z",
     "iopub.status.idle": "2024-10-22T22:07:57.728899Z",
     "shell.execute_reply": "2024-10-22T22:07:57.728323Z",
     "shell.execute_reply.started": "2024-10-22T22:07:53.443786Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nubert.datasets import AmountDataset\n",
    "from nubert.config import AmountConfig, TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c417667-d954-4092-90cd-476772d58d9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T22:07:57.732606Z",
     "iopub.status.busy": "2024-10-22T22:07:57.731970Z",
     "iopub.status.idle": "2024-10-22T22:07:57.736627Z",
     "shell.execute_reply": "2024-10-22T22:07:57.735761Z",
     "shell.execute_reply.started": "2024-10-22T22:07:57.732584Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(dataset, val_size=0.1, seed=42):\n",
    "    train, val = train_test_split(dataset, test_size=val_size, random_state=seed)\n",
    "    \n",
    "    return train, val\n",
    "\n",
    "def create_hf_dataset(data):\n",
    "    input_ids = [example[\"input_ids\"] for example in data]\n",
    "    labels = [example[\"label\"] for example in data]\n",
    "    return Dataset.from_dict({\"input_ids\": input_ids, \"labels\": labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a92cb4-f8ce-47dd-9a68-193b3e289a0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T22:07:57.737796Z",
     "iopub.status.busy": "2024-10-22T22:07:57.737591Z",
     "iopub.status.idle": "2024-10-22T22:07:58.039726Z",
     "shell.execute_reply": "2024-10-22T22:07:58.038952Z",
     "shell.execute_reply.started": "2024-10-22T22:07:57.737796Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "def train_model(\n",
    "    dataset,\n",
    "    config: AmountConfig,\n",
    "    num_labels: int,\n",
    "    ):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.model_name,\n",
    "        num_labels=num_labels,\n",
    "    )\n",
    "    tokenizer = dataset.tokenizer.base_tokenizer\n",
    "\n",
    "    train_data, val_data = split_dataset(dataset)\n",
    "\n",
    "    train_dataset = create_hf_dataset(train_data)\n",
    "    val_dataset = create_hf_dataset(val_data)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        **config.trainer.model_dump()\n",
    "    )\n",
    "    \n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(config.trainer.output_dir)\n",
    "    wandb.finish()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092ccf6a-9d8b-4ffa-8778-05bd32cdc961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T22:07:58.041360Z",
     "iopub.status.busy": "2024-10-22T22:07:58.041176Z",
     "iopub.status.idle": "2024-10-22T22:33:22.193849Z",
     "shell.execute_reply": "2024-10-22T22:33:22.193149Z",
     "shell.execute_reply.started": "2024-10-22T22:07:58.041342Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/nubert/datasets/amount_dataset.py:74: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path.join(root, f\"{fname}.csv\"))\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [11:33<00:00,  6.14s/it]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /notebooks/nubank/models/nubert/nubert-distil-transactions-7-stride-1-randomize-False-bins-15 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrafaelmcelente\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/nubank/nubert/analyses/amount-2014-2014/wandb/run-20241022_222002-rj5cw9jy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rafaelmcelente/amount/runs/rj5cw9jy' target=\"_blank\">amount-transactions-7-stride-1-randomize-False-bins-15</a></strong> to <a href='https://wandb.ai/rafaelmcelente/amount' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rafaelmcelente/amount' target=\"_blank\">https://wandb.ai/rafaelmcelente/amount</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rafaelmcelente/amount/runs/rj5cw9jy' target=\"_blank\">https://wandb.ai/rafaelmcelente/amount/runs/rj5cw9jy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3010' max='3010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3010/3010 13:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.352500</td>\n",
       "      <td>1.412897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def6c10c9cca4cbb8e3bbc44a189f4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='80.228 MB of 255.652 MB uploaded\\r'), FloatProgress(value=0.31381597855168114, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–</td></tr><tr><td>eval/runtime</td><td>â–</td></tr><tr><td>eval/samples_per_second</td><td>â–</td></tr><tr><td>eval/steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–â–†â–„â–„â–‚â–†â–ƒâ–‚â–â–ƒâ–„â–ˆâ–„â–ƒâ–‚â–â–ƒâ–ƒâ–‚â–ƒâ–…â–‚â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–„â–ƒâ–„â–…â–ƒâ–ƒ</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–…â–…â–„â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–â–â–‚â–‚â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.4129</td></tr><tr><td>eval/runtime</td><td>29.2368</td></tr><tr><td>eval/samples_per_second</td><td>732.022</td></tr><tr><td>eval/steps_per_second</td><td>11.458</td></tr><tr><td>total_flos</td><td>1.943389417075164e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>3010</td></tr><tr><td>train/grad_norm</td><td>4.37164</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.3525</td></tr><tr><td>train_loss</td><td>1.4943</td></tr><tr><td>train_runtime</td><td>791.2128</td></tr><tr><td>train_samples_per_second</td><td>243.439</td></tr><tr><td>train_steps_per_second</td><td>3.804</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amount-transactions-7-stride-1-randomize-False-bins-15</strong> at: <a href='https://wandb.ai/rafaelmcelente/amount/runs/rj5cw9jy' target=\"_blank\">https://wandb.ai/rafaelmcelente/amount/runs/rj5cw9jy</a><br/> View project at: <a href='https://wandb.ai/rafaelmcelente/amount' target=\"_blank\">https://wandb.ai/rafaelmcelente/amount</a><br/>Synced 5 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241022_222002-rj5cw9jy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"amount\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "\n",
    "\n",
    "num_transactions_to_test = [5, 7, 10]\n",
    "stride_to_test = [1, 2]\n",
    "num_bins_to_test = [15, 20]\n",
    "randomized_to_test = [False, True]\n",
    "\n",
    "for num_transactions in num_transactions_to_test:\n",
    "    for stride in stride_to_test:\n",
    "        for num_bins in num_bins_to_test:\n",
    "            for randomize_column_order in randomized_to_test:\n",
    "                model_name = f\"nubert-distil-transactions-{num_transactions}-stride-{stride}-randomize-{str(randomize_column_order)}-bins-{num_bins}\"\n",
    "                model_name = os.path.join(\"/notebooks/nubank/models/nubert\", model_name)\n",
    "                trainer_config = TrainerConfig(\n",
    "                    per_device_train_batch_size = 64,\n",
    "                    per_device_eval_batch_size = 64,\n",
    "                    output_dir = \"/notebooks/nubank/models/amount\"\n",
    "                )\n",
    "                config = AmountConfig(\n",
    "                    model_name = model_name,\n",
    "                    dataset_path = \"/notebooks/nubank/nubert/analyses/amount-2014-2014\",\n",
    "                    file_name = \"amount_raw\",\n",
    "                    num_transactions = num_transactions,\n",
    "                    stride = stride,\n",
    "                    num_bins = num_bins,\n",
    "                    trainer=trainer_config,\n",
    "                )\n",
    "                full_dataset = AmountDataset.from_config(config)\n",
    "                train_model(dataset=full_dataset, config=config, num_labels=num_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc60b30-edb7-405b-8d27-1b5aaf5caaf7",
   "metadata": {},
   "source": [
    "## Baseline: Fine-tuning without the pre-training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4101fdb0-249f-4688-b19b-b89826597de4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T22:33:22.209238Z",
     "iopub.status.busy": "2024-10-22T22:33:22.208760Z",
     "iopub.status.idle": "2024-10-22T22:33:22.217770Z",
     "shell.execute_reply": "2024-10-22T22:33:22.217115Z",
     "shell.execute_reply.started": "2024-10-22T22:33:22.209213Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"amount\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "\n",
    "\n",
    "num_transactions = 7\n",
    "stride = 1\n",
    "num_bins = 15\n",
    "randomized = False\n",
    "\n",
    "\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "trainer_config = TrainerConfig(\n",
    "    per_device_train_batch_size = 128,\n",
    "    per_device_eval_batch_size = 128,\n",
    "    output_dir = \"/notebooks/nubank/models/amount\"\n",
    ")\n",
    "config = AmountConfig(\n",
    "    model_name = model_name,\n",
    "    dataset_path = \"/notebooks/nubank/nubert/analyses/amount-2014-2014\",\n",
    "    file_name = \"amount_raw\",\n",
    "    num_transactions = num_transactions,\n",
    "    stride = stride,\n",
    "    num_bins = num_bins,\n",
    "    trainer=trainer_config,\n",
    ")\n",
    "full_dataset = AmountDataset.from_config(config)\n",
    "train_model(dataset=full_dataset, config=config, num_labels=num_bins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
